# INSTRUCTION.md

**Project**  
Spectrally Adaptive Mirror Descent with Streaming Sketches  
**Goal**  
Author productionâ€‘quality Python code to run rigorous experiments that **directly** support the paper â€œSpectrally Adaptive Mirror Descent with Streaming Sketches: Instanceâ€‘Optimal Regret and Fast Ratesâ€. You are a coding agent. You will implement algorithms, data generators, metrics, sanity checks, unit tests, ablations, and a final notebook that reproduces all figures and tables for the paper.

You must prioritize correctness, reproducibility, and traceability. Every claim drawn in the paper must be backed by a corresponding experiment, metric, or inequality check in this repo.

---

## 1. Scope and Acceptance Criteria

1. Implement the proposed method **SAMDâ€‘SS**: Spectrally Adaptive Mirror Descent using a streaming sketch preconditioner.
2. Implement baselines: SGD, AdaGradâ€‘Diag, AdaGradâ€‘Full (for small dimensions), and ONSâ€‘Diag.  
   AdaGradâ€‘Full and exact logâ€‘det computations are only required when the dimension is small enough to run reliably on CPU.
3. Provide synthetic and realâ€‘data experiments that validate four core theoretical statements:

   3.1 Elliptical potential control  
   \[
   S_T \triangleq \sum_{t=1}^{T} g_t^\top H_t^{-1} g_t 
   \;\le\; \frac{2}{1-\epsilon}\,\log\frac{\det(\lambda I + G_T)}{\det(\lambda I)} \quad \text{up to numerical tolerance.}
   \]

   3.2 Instanceâ€‘dependent regret bound with effective dimension scaling  
   \[
   \mathrm{Reg}_T(x^\star) \le D \sqrt{\frac{2\lambda}{1-\epsilon}\,\log\frac{\det(\lambda I + G_T)}{\det(\lambda I)}}.
   \]

   3.3 Fast rates under strong convexity using \(\eta_t=\tfrac{1}{\alpha t}\) with clear \(\log T\) growth.

   3.4 Sketch inflation behaves as predicted when the sketch size \(r\) varies, and the empirical inflation approaches \(1/(1-\widehat{\epsilon}_{\mathrm{ridge}})\).

4. Produce a single executable Jupyter notebook that:
   4.1 Runs all experiments deterministically with fixed seeds.  
   4.2 Generates all plots and tables saved under `artifacts/figures` and `artifacts/tables`.  
   4.3 Verifies each inequality or bound with explicit pass or fail checks.  
   4.4 Exports LaTeXâ€‘ready tables and PDFs of figures.

5. Provide unit tests and numerical sanity checks that run via `pytest` and must pass before the notebook runs.

6. Reproducibility:
   6.1 Single `environment.yml` with pinned versions.  
   6.2 A topâ€‘level `run_all.sh` that sets seeds, runs unit tests, launches sweeps, and builds the notebook outputs.

Failure to meet any acceptance item is a fail.

---

## 2. Repository Layout

Create the following structure exactly.

â”œâ”€â”€ environment.yml
â”œâ”€â”€ README.md
â”œâ”€â”€ INSTRUCTION.md
â”œâ”€â”€ run_all.sh
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ algorithms/
â”‚ â”‚ â”œâ”€â”€ samd_ss.py
â”‚ â”‚ â”œâ”€â”€ sgd.py
â”‚ â”‚ â”œâ”€â”€ adagrad_diag.py
â”‚ â”‚ â”œâ”€â”€ adagrad_full.py
â”‚ â”‚ â””â”€â”€ ons_diag.py
â”‚ â”œâ”€â”€ sketches/
â”‚ â”‚ â”œâ”€â”€ frequent_directions.py
â”‚ â”‚ â”œâ”€â”€ oja_sketch.py
â”‚ â”‚ â””â”€â”€ randomized_svd_stream.py
â”‚ â”œâ”€â”€ data/
â”‚ â”‚ â”œâ”€â”€ synthetic.py
â”‚ â”‚ â””â”€â”€ real.py
â”‚ â”œâ”€â”€ losses/
â”‚ â”‚ â”œâ”€â”€ squared.py
â”‚ â”‚ â””â”€â”€ logistic.py
â”‚ â”œâ”€â”€ metrics/
â”‚ â”‚ â”œâ”€â”€ regret.py
â”‚ â”‚ â”œâ”€â”€ logdet.py
â”‚ â”‚ â”œâ”€â”€ stability.py
â”‚ â”‚ â””â”€â”€ complexity.py
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â”œâ”€â”€ config.py
â”‚ â”‚ â”œâ”€â”€ logging.py
â”‚ â”‚ â”œâ”€â”€ linalg.py
â”‚ â”‚ â”œâ”€â”€ projections.py
â”‚ â”‚ â”œâ”€â”€ reproducibility.py
â”‚ â”‚ â””â”€â”€ timers.py
â”‚ â””â”€â”€ experiments/
â”‚ â”œâ”€â”€ run_experiment.py
â”‚ â”œâ”€â”€ sweep.py
â”‚ â””â”€â”€ configs/
â”‚ â”œâ”€â”€ 01_effective_dimension.yaml
â”‚ â”œâ”€â”€ 02_sketch_inflation.yaml
â”‚ â”œâ”€â”€ 03_fast_rates.yaml
â”‚ â”œâ”€â”€ 04_stability_generalization.yaml
â”‚ â””â”€â”€ 05_large_scale.yaml
â”œâ”€â”€ tests/
â”‚ â”œâ”€â”€ test_algorithms.py
â”‚ â”œâ”€â”€ test_sketches.py
â”‚ â”œâ”€â”€ test_metrics.py
â”‚ â””â”€â”€ test_sanity.py
â”œâ”€â”€ artifacts/
â”‚ â”œâ”€â”€ logs/
â”‚ â”œâ”€â”€ figures/
â”‚ â””â”€â”€ tables/
â””â”€â”€ notebooks/
â””â”€â”€ main_experiments.ipynb


---

## 3. Environment and Dependencies

1. Provide `environment.yml` with pinned versions that are widely available on Linux x86_64. Example:


name: samd-ss
channels: [conda-forge, defaults]
dependencies:
- python=3.10
- numpy=1.26.*
- scipy=1.11.*
- scikit-learn=1.4.*
- matplotlib=3.8.*
- pandas=2.2.*
- numba=0.58.*
- tqdm=4.66.*
- jupyterlab=4.2.*
- ipywidgets=8.1.*
- pytest=7.4.*
- psutil=5.9.*


2. All scripts must run on CPU by default. If GPU is available, do not change numerical results.

3. Enforce `PYTHONHASHSEED`, NumPy RNG seeds, and `random.seed` through `src/utils/reproducibility.py`.

---

## 4. Mathematical Objects and Notation Mapping

You will implement the following objects.

1. Cumulative covariance  
\( G_t = \sum_{s=1}^{t} g_s g_s^\top \)

2. Metric  
\( H_t = \lambda I + \widetilde{G}_{t-1} \)

3. Elliptical term  
\( q_t = g_t^\top H_t^{-1} g_t \)

4. Potential  
\( \Phi_T(\lambda) = \log \frac{\det(\lambda I + G_T)}{\det(\lambda I)} \)

5. Effective dimension  
\( d_{\mathrm{eff}}(\lambda;G_T) = \mathrm{tr}\big(G_T(G_T+\lambda I)^{-1}\big) \)

6. Sketch quality proxy with ridge  
\( \widehat{\epsilon}_{\mathrm{ridge}} = \frac{\| \widetilde{G}_T - G_T \|_2}{\lambda + \lambda_{\min}(G_T)} \) clipped to \([0,1)\).

---

## 5. Core Algorithms and Interfaces

### 5.1 SAMD-SS

File `src/algorithms/samd_ss.py`

Implement a class `SAMDSS` with the following interface:

```python
class SAMDSS:
 def __init__(self, dim, lambda_ridge, step_schedule, sketch_backend, sketch_kwargs,
              constraint=None, alpha_strong_convexity=None):
     """
     dim: int
     lambda_ridge: float, must satisfy lambda_ridge >= max_grad_norm**2 after warmup
     step_schedule: callable t -> eta_t
     sketch_backend: object exposing .update(g) and .matrix() -> B_t B_t^T or an implicit operator
     sketch_kwargs: dict forwarded to the sketch constructor
     constraint: dict or None, e.g. {"type": "l2_ball", "radius": R}
     alpha_strong_convexity: float or None, if using 1/(alpha t) schedule in fast-rate experiments
     """
 def reset(self, x0=None, rng=None): ...
 def step(self, grad, t): ...
 def get_state(self): ...


Update rule for unconstrained case:

ğ‘¥
ğ‘¡
+
1
=
ğ‘¥
ğ‘¡
âˆ’
ğœ‚
ğ‘¡
ğ»
ğ‘¡
âˆ’
1
ğ‘”
ğ‘¡
x
t+1
	â€‹

=x
t
	â€‹

âˆ’Î·
t
	â€‹

H
t
âˆ’1
	â€‹

g
t
	â€‹


with

ğ»
ğ‘¡
âˆ’
1
=
ğœ†
âˆ’
1
ğ¼
âˆ’
ğœ†
âˆ’
1
ğµ
(
ğµ
âŠ¤
ğµ
+
ğœ†
ğ¼
ğ‘Ÿ
)
âˆ’
1
ğµ
âŠ¤
ğœ†
âˆ’
1
H
t
âˆ’1
	â€‹

=Î»
âˆ’1
Iâˆ’Î»
âˆ’1
B(B
âŠ¤
B+Î»I
r
	â€‹

)
âˆ’1
B
âŠ¤
Î»
âˆ’1

computed through the Woodbury identity using the sketch matrix 
ğµ
âˆˆ
ğ‘…
ğ‘‘
Ã—
ğ‘Ÿ
BâˆˆR
dÃ—r
.

If constraint is provided as an 
â„“
2
â„“
2
	â€‹

 ball, perform the projection

ğ‘¥
ğ‘¡
+
1
=
arg
â¡
min
â¡
âˆ¥
ğ‘¥
âˆ¥
2
â‰¤
ğ‘…
1
2
âˆ¥
ğ‘¥
âˆ’
(
ğ‘¥
ğ‘¡
âˆ’
ğœ‚
ğ‘¡
ğ»
ğ‘¡
âˆ’
1
ğ‘”
ğ‘¡
)
âˆ¥
ğ»
ğ‘¡
2
x
t+1
	â€‹

=arg
âˆ¥xâˆ¥
2
	â€‹

â‰¤R
min
	â€‹

2
1
	â€‹

âˆ¥xâˆ’(x
t
	â€‹

âˆ’Î·
t
	â€‹

H
t
âˆ’1
	â€‹

g
t
	â€‹

)âˆ¥
H
t
	â€‹

2
	â€‹


using the scalar dual search detailed in src/utils/projections.py via a monotone root-find with closed-form matvecs through Woodbury.

The class must:

Track 
ğ‘
ğ‘¡
q
t
	â€‹

, cumulative 
ğ‘†
ğ‘‡
=
âˆ‘
ğ‘
ğ‘¡
S
T
	â€‹

=âˆ‘q
t
	â€‹

, and a rolling estimate of 
max
â¡
âˆ¥
ğ‘”
ğ‘¡
âˆ¥
2
maxâˆ¥g
t
	â€‹

âˆ¥
2
	â€‹

.

Expose instrumentation hooks to record per-step wall time, memory, and sketch diagnostics.

5.2 Baselines

SGD with fixed and cosine schedules.

AdaGradDiag with diagonal accumulator and standard update.

AdaGradFull using exact 
ğ»
ğ‘¡
=
ğœ†
ğ¼
+
ğº
ğ‘¡
âˆ’
1
H
t
	â€‹

=Î»I+G
tâˆ’1
	â€‹

 with Cholesky solves for small 
ğ‘‘
d.

ONS-Diag using a diagonal approximation.

All baselines must share a common interface with fit_one_pass(dataset, loss) returning logs.

5.3 Sketch Backends

Implement three choices behind a unified interface:

class Sketch:
    def update(self, g: np.ndarray): ...
    def matrix(self): ...
    def inv_h_matvec(self, v, lambda_ridge): ...
    def diagnostics(self) -> dict: ...
FrequentDirections with rank r, deterministic and streaming. Maintain a thin SVD and shrink the smallest singular values.
Provide an analytic upper bound proxy for 
âˆ¥
ğº
~
âˆ’
ğº
âˆ¥
2
âˆ¥
G
âˆ’Gâˆ¥
2
	â€‹

 using the tail energy stored during shrink steps.

OjaSketch incremental subspace estimation with learning rate schedule and periodic re-orthogonalization. Return a PSD approx 
ğº
~
â‰ˆ
ğµ
ğµ
âŠ¤
G
â‰ˆBB
âŠ¤
.

RandomizedSVDStream with mini-batch sketching and power iterations.

All sketches must provide the matrix 
ğµ
B or an operator sufficient for Woodbury calculations and must record:

Current rank, spectral tail estimate, and approximate 
ğœ–
^
r
i
d
g
e
Ïµ
ridge
	â€‹

.

5.4 Losses and Gradients

Squared loss: 
ğ‘“
ğ‘¡
(
ğ‘¥
)
=
1
2
(
ğ‘¦
ğ‘¡
âˆ’
ğ‘
ğ‘¡
âŠ¤
ğ‘¥
)
2
+
ğœ†
reg
2
âˆ¥
ğ‘¥
âˆ¥
2
2
f
t
	â€‹

(x)=
2
1
	â€‹

(y
t
	â€‹

âˆ’a
t
âŠ¤
	â€‹

x)
2
+
2
Î»
reg
	â€‹

	â€‹

âˆ¥xâˆ¥
2
2
	â€‹


Gradient: 
ğ‘”
ğ‘¡
=
âˆ’
(
ğ‘¦
ğ‘¡
âˆ’
ğ‘
ğ‘¡
âŠ¤
ğ‘¥
ğ‘¡
)
ğ‘
ğ‘¡
+
ğœ†
reg
ğ‘¥
ğ‘¡
g
t
	â€‹

=âˆ’(y
t
	â€‹

âˆ’a
t
âŠ¤
	â€‹

x
t
	â€‹

)a
t
	â€‹

+Î»
reg
	â€‹

x
t
	â€‹

.

Logistic loss with L2: 
ğ‘“
ğ‘¡
(
ğ‘¥
)
=
log
â¡
(
1
+
exp
â¡
(
âˆ’
ğ‘¦
ğ‘¡
ğ‘
ğ‘¡
âŠ¤
ğ‘¥
)
)
+
ğœ†
reg
2
âˆ¥
ğ‘¥
âˆ¥
2
2
f
t
	â€‹

(x)=log(1+exp(âˆ’y
t
	â€‹

a
t
âŠ¤
	â€‹

x))+
2
Î»
reg
	â€‹

	â€‹

âˆ¥xâˆ¥
2
2
	â€‹

.

Provide numerically stable implementations with clipping for logits. For strong-convexity experiments use 
ğ›¼
=
ğœ†
reg
Î±=Î»
reg
	â€‹

 as a Euclidean lower bound and report the schedule 
ğœ‚
ğ‘¡
=
1
/
(
ğ›¼
ğ‘¡
)
Î·
t
	â€‹

=1/(Î±t).

6. Data Generators
6.1 Synthetic with Controlled Spectrum

src/data/synthetic.py must support:

Low-rank subspace with ambient dimension 
ğ‘‘
d, intrinsic rank 
ğ‘˜
k, eigenvalues 
{
ğœ
ğ‘–
}
{Ïƒ
i
	â€‹

} decaying as power-law or exponential.
Generate features 
ğ‘
ğ‘¡
=
ğ‘ˆ
Î£
1
/
2
ğ‘§
ğ‘¡
+
ğœ
i
s
o
ğœ‰
ğ‘¡
a
t
	â€‹

=UÎ£
1/2
z
t
	â€‹

+Ïƒ
iso
	â€‹

Î¾
t
	â€‹

 with 
ğ‘§
ğ‘¡
,
ğœ‰
ğ‘¡
âˆ¼
ğ‘
(
0
,
ğ¼
)
z
t
	â€‹

,Î¾
t
	â€‹

âˆ¼N(0,I).

Labels: linear model 
ğ‘¦
ğ‘¡
=
s
i
g
n
(
ğ‘¤
â‹†
âŠ¤
ğ‘
ğ‘¡
+
ğœ€
)
y
t
	â€‹

=sign(w
â‹†
âŠ¤
	â€‹

a
t
	â€‹

+Îµ) or real-valued for squared loss.

ğ‘¤
â‹†
w
â‹†
	â€‹

 is drawn in the same low-rank subspace.

Expose knobs: 
ğ‘‘
,
ğ‘˜
,
ğ‘‡
,
decay_rate
,
ğœ
i
s
o
,
snr
,
seed
d,k,T,decay_rate,Ïƒ
iso
	â€‹

,snr,seed.

6.2 Real Data

src/data/real.py may use scikit-learn fetchers with offline caching to ~/.cache/samd_ss/. Provide at least one classification dataset and one regression dataset of moderate dimension 
ğ‘‘
âˆˆ
[
100
,
2000
]
dâˆˆ[100,2000]. If download fails, experiments must gracefully skip with a warning and continue with synthetic data.

7. Metrics, Bounds, and Diagnostics

Implement in src/metrics/:

Regret to the best in hindsight
Solve 
ğ‘¥
ğ‘‡
â‹†
=
arg
â¡
min
â¡
ğ‘¥
âˆˆ
ğ¾
âˆ‘
ğ‘¡
=
1
ğ‘‡
ğ‘“
ğ‘¡
(
ğ‘¥
)
x
T
â‹†
	â€‹

=argmin
xâˆˆK
	â€‹

âˆ‘
t=1
T
	â€‹

f
t
	â€‹

(x) using L-BFGS-B with optional projection to 
â„“
2
â„“
2
	â€‹

 ball. Then compute 
R
e
g
ğ‘‡
(
ğ‘¥
ğ‘‡
â‹†
)
Reg
T
	â€‹

(x
T
â‹†
	â€‹

). For logistic, stop when gradient norm is less than 1e-8 or relative decrease less than 1e-10.

Elliptical potential
Track 
ğ‘†
ğ‘‡
=
âˆ‘
ğ‘¡
=
1
ğ‘‡
ğ‘
ğ‘¡
S
T
	â€‹

=âˆ‘
t=1
T
	â€‹

q
t
	â€‹

.
For small 
ğ‘‘
d, compute 
Î¦
ğ‘‡
(
ğœ†
)
Î¦
T
	â€‹

(Î») exactly using np.linalg.slogdet.
For large 
ğ‘‘
d, implement Hutch++ log-det approximation with Rademacher probes and Lanczos; provide absolute and relative error estimates.

Effective dimension
Compute 
ğ‘‘
e
f
f
(
ğœ†
;
ğº
ğ‘‡
)
d
eff
	â€‹

(Î»;G
T
	â€‹

) exactly when feasible or via trace estimators.

Stability
Replace-one stability: run paired experiments on datasets that differ in exactly one example but share the same RNG. Report the maximum parameter deviation over time and the test generalization gap difference.

Complexity
Measure per-step wall time, cumulative time, memory via psutil and tracemalloc. Validate the observed cost trends with 
ğ‘‚
(
ğ‘‘
ğ‘Ÿ
+
ğ‘Ÿ
3
)
O(dr+r
3
).

Sketch inflation
Estimate 
ğœ–
^
r
i
d
g
e
Ïµ
ridge
	â€‹

. Report the empirical ratio 
ğœŒ
ğ‘‡
=
ğ‘†
ğ‘‡
sketch
ğ‘†
ğ‘‡
full
Ï
T
	â€‹

=
S
T
full
	â€‹

S
T
sketch
	â€‹

	â€‹

 where the denominator uses exact 
ğ»
ğ‘¡
H
t
	â€‹

 in small-d runs. Compare 
ğœŒ
ğ‘‡
Ï
T
	â€‹

 to 
1
/
(
1
âˆ’
ğœ–
^
r
i
d
g
e
)
1/(1âˆ’
Ïµ
ridge
	â€‹

).

All metrics must be persisted as CSV and JSON lines in artifacts/logs/.

8. Experiments to Implement

Each experiment is configured by a YAML under src/experiments/configs/. For every config below, run 10 seeds unless stated otherwise and report mean Â± standard error with 95 percent confidence intervals where applicable.

8.1 E1: Effective Dimension Scaling

Goal: validate instance-dependent behavior on synthetic data with power-law spectra.

Vary decay_rate âˆˆ {0.5, 1.0, 1.5, 2.0} which alters 
ğ‘‘
e
f
f
d
eff
	â€‹

.

Fixed 
ğ‘‘
=
500
d=500, 
ğ‘‡
=
10000
T=10000, lambda_ridge selected after warmup as 
â‰¥
max
â¡
ğ‘¡
âˆ¥
ğ‘”
ğ‘¡
âˆ¥
2
â‰¥max
t
	â€‹

âˆ¥g
t
	â€‹

âˆ¥
2
.

Methods: SAMD-SS, AdaGrad-Diag, SGD.

Outputs:

R
e
g
ğ‘‡
Reg
T
	â€‹

 vs 
ğ‘‡
T

Î¦
ğ‘‡
Î¦
T
	â€‹

 vs 
ğ‘‡
T

R
e
g
ğ‘‡
/
Î¦
ğ‘‡
Reg
T
	â€‹

/
Î¦
T
	â€‹

	â€‹

 vs 
ğ‘‡
T

Table comparing final regret across methods and mapping to 
ğ‘‘
e
f
f
d
eff
	â€‹

.

8.2 E2: Sketch Inflation

Goal: quantify regret and potential inflation when sketch size 
ğ‘Ÿ
r changes.

Vary 
ğ‘Ÿ
âˆˆ
{
8
,
16
,
32
,
64
,
128
}
râˆˆ{8,16,32,64,128} at 
ğ‘‘
=
2000
d=2000, 
ğ‘‡
=
20000
T=20000.

Methods: SAMD-SS with FrequentDirections, OjaSketch, RandomizedSVDStream.

Outputs:

ğ‘†
ğ‘‡
S
T
	â€‹

 vs 
ğ‘‡
T for each 
ğ‘Ÿ
r.

Plot of 
ğœŒ
ğ‘‡
Ï
T
	â€‹

 and 
1
/
(
1
âˆ’
ğœ–
^
r
i
d
g
e
)
1/(1âˆ’
Ïµ
ridge
	â€‹

) as functions of 
ğ‘Ÿ
r.

Table: time and memory by 
ğ‘Ÿ
r.

8.3 E3: Fast Rates under Strong Convexity

Goal: verify 
log
â¡
ğ‘‡
logT regret behavior with 
ğœ‚
ğ‘¡
=
1
/
(
ğ›¼
ğ‘¡
)
Î·
t
	â€‹

=1/(Î±t).

Use logistic regression with L2 regularization on synthetic data and one real dataset.

Methods: SAMD-SS, AdaGrad-Diag, SGD with tuned schedules.

Outputs:

R
e
g
ğ‘‡
Reg
T
	â€‹

 vs 
log
â¡
ğ‘‡
logT showing linear trend.

Test loss vs passes.

Bound check: cumulative bound computed online compared to observed regret.